{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-30T17:36:33.178484500Z",
     "start_time": "2023-06-30T17:36:29.797374900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from numpy import ndarray\n",
    "\n",
    "from torch import Tensor, zeros, empty, tanh, eye\n",
    "from torch.linalg import eigvals, pinv\n",
    "\n",
    "from LAB3_1.utils import make_sequence\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EchoStateNetwork_base:\n",
    "    def __init__(self,input_dim:int, hidden_dim:int, leakage_rate:float,\n",
    "                 spectral_radius:float, omega:float):\n",
    "\n",
    "        self.leakage_rate = leakage_rate\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        Wx = torch.rand((hidden_dim,input_dim))\n",
    "        Wh = torch.rand((hidden_dim, hidden_dim))\n",
    "        b = torch.rand(hidden_dim)\n",
    "\n",
    "        self.Wx =  (Wx * 2 -1) * omega\n",
    "        self.b = (b * 2 -1) * omega\n",
    "\n",
    "        Wh = Wh * 2 - 1\n",
    "        self.Wh = Wh * spectral_radius / self.spectral_radius(Wh)\n",
    "\n",
    "    def gpu(self, device:str):\n",
    "        self.Wx = self.Wx.to(device)\n",
    "        self.Wh = self.Wh.to(device)\n",
    "        self.b = self.b.to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectral_radius(matrix) -> float:\n",
    "        eigenvalues = eigvals(matrix)\n",
    "        max_magnitude =torch.max(torch.abs(eigenvalues))\n",
    "        return max_magnitude\n",
    "\n",
    "    def resevoir(self, x:Tensor, h0:Tensor=None):\n",
    "\n",
    "        h_stack = empty(x.size(0), self.hidden_dim)\n",
    "\n",
    "        if h0 is None:\n",
    "             h0 = zeros(self.hidden_dim)\n",
    "\n",
    "        h_stack[0] = h0.clone()\n",
    "\n",
    "        for step, x_ in enumerate(x):\n",
    "            z = self.Wx @ x_.T + self.Wh @ h_stack[step].T + self.b\n",
    "            h_stack[step] = (1 - self.leakage_rate) * h_stack[step] + self.leakage_rate * tanh(z.T)\n",
    "\n",
    "        return h_stack\n",
    "\n",
    "class ESN_Seq2Seq(EchoStateNetwork_base):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, leakage_rate: float,\n",
    "                 tikhonov:float, spectral_radius: float, omega: float):\n",
    "\n",
    "        super().__init__(input_dim, hidden_dim, leakage_rate, spectral_radius, omega)\n",
    "\n",
    "        self.Wo = None\n",
    "        self.hidden_cache = None\n",
    "        self.tikhonov = tikhonov\n",
    "\n",
    "    @staticmethod\n",
    "    def MSE(y: Tensor, y_pred: Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Mean square error\n",
    "        :param y: Target\n",
    "        :param y_pred: Predicted target\n",
    "        \"\"\"\n",
    "        return torch.pow((y - y_pred), 2).mean()\n",
    "\n",
    "    def fit(self, x:Tensor, y:Tensor, transient:int):\n",
    "\n",
    "        h_stack = self.resevoir(x)\n",
    "        h_stack, y = h_stack[transient:], y[transient:]\n",
    "\n",
    "        I = eye(h_stack.shape[1])\n",
    "        self.Wo = pinv(h_stack.T @ h_stack + self.tikhonov * I) @ h_stack.T @ y\n",
    "        self.hidden_cache = h_stack\n",
    "\n",
    "\n",
    "    def predict(self, x:Tensor, y:Tensor=None, h0:Tensor=None):\n",
    "        h_stack = self.resevoir(x, h0)\n",
    "        y_pred = h_stack @ self.Wo\n",
    "\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = self.MSE(y, y_pred)\n",
    "\n",
    "        output = h_stack, y_pred\n",
    "        return  (loss,) + output if loss is not None else output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T17:36:33.216277300Z",
     "start_time": "2023-06-30T17:36:33.194081500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "\n",
    "    def __init__(self, parameters_grid:dict, tr:ndarray, dev:ndarray, dt_type:str):\n",
    "\n",
    "        all_configs = [dict(zip(parameters_grid.keys(), configs)) for configs in product(*parameters_grid.values())]\n",
    "\n",
    "        print(\"Number of configurations to try: \",len(all_configs))\n",
    "        \"\"\"\n",
    "        Returns the performance in each configuration:\n",
    "            rank = a list of results for each configuration\n",
    "            loss = training loss history of the best model\n",
    "        \"\"\"\n",
    "        rank = self.run(tr, dev, all_configs, dt_type)\n",
    "\n",
    "        # we sort by validation loss\n",
    "        rank = sorted(rank, key=lambda conf: conf[2])\n",
    "\n",
    "        print(\"\\nThe best solution in \", rank[0])\n",
    "        self.best_config = rank[0][0]\n",
    "\n",
    "    @staticmethod\n",
    "    def run(tr:ndarray, dev:ndarray, configs:list, dt_type:str):\n",
    "        \"\"\"\n",
    "        In the grid search, we explore all configurations provided and try to find the best\n",
    "        hyperparameter configuration using the training set to train the model and the validation\n",
    "        set to compare the performance among all models instantiated by configurations.\n",
    "        \"\"\"\n",
    "\n",
    "        rank = [] # keep in track the configuration and the corresponding performance\n",
    "\n",
    "        # we save the best trained model and the training loss history during the epochs\n",
    "        best_dev_loss = sys.maxsize\n",
    "\n",
    "        tr = make_sequence(tr, 1, dt_type)\n",
    "        dev = make_sequence(dev, 1, dt_type)\n",
    "\n",
    "        for config in tqdm(configs): # try each configuration\n",
    "\n",
    "            # With the same hyperparameter, we perform 3 different \"trainings\" and\n",
    "            # we evaluate the configuration as average of this training\n",
    "            losses = multiple_train(tr, config, [dev])\n",
    "            rank.append((config, round(losses[0].item(), 6), round(losses[1].item(), 6)))\n",
    "            # we keep the best model\n",
    "            if best_dev_loss > losses[1]:\n",
    "                best_dev_loss = losses[1]\n",
    "\n",
    "        return rank\n",
    "\n",
    "def multiple_train(tr:tuple[Tensor,Tensor], config:dict,\n",
    "                   dts:list[tuple]=None, return_model:bool=False):\n",
    "    \"\"\"\n",
    "    Perform the training different time, and we keep the average loss\n",
    "    :param tr: Training dataset using for fitting the readout\n",
    "    :param config: Hyperparameters\n",
    "    :param dts: other datasets to evaluate\n",
    "    :param return_model: if true return the model\n",
    "    \"\"\"\n",
    "    trains, trainer = [], None\n",
    "    for _ in range(3):\n",
    "        losses = []\n",
    "\n",
    "        trainer = ESN_Seq2Seq(1, hidden_dim=config[\"units\"],\n",
    "                              omega=config[\"omega\"],\n",
    "                              spectral_radius=config[\"spectral\"],\n",
    "                              tikhonov=config[\"lambda\"],\n",
    "                              leakage_rate=config[\"leakage\"])\n",
    "\n",
    "        trainer.fit(*tr, transient=config[\"transient\"])\n",
    "\n",
    "        tr_loss, hs, _ = trainer.predict(*tr)\n",
    "\n",
    "        losses.append(tr_loss)\n",
    "        if dts is not None:\n",
    "            for dt in dts:\n",
    "                loss, hs, _ =  trainer.predict(*dt, hs[-1])\n",
    "                losses.append(loss)\n",
    "\n",
    "        trains.append(losses)\n",
    "    print(trains)\n",
    "    trains = torch.tensor(trains).mean(axis=0)\n",
    "    return (trains, trainer) if return_model else trains\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T17:36:33.231880900Z",
     "start_time": "2023-06-30T17:36:33.231880900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "source2 = pd.read_csv(\"./../sources/MG17.csv\", header=None).T.to_numpy()\n",
    "tr_dataset, dev_dataset, ts_dataset = source2[:4000], source2[4000:5000], source2[5000:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T17:36:33.513683Z",
     "start_time": "2023-06-30T17:36:33.231880900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of configurations to try:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]C:\\Users\\Andrea\\AppData\\Local\\Temp\\ipykernel_4588\\106195679.py:39: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  z = self.Wx @ x_.T + self.Wh @ h_stack[step].T + self.b\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.svd: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 23).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_LinAlgError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 9\u001B[0m\n\u001B[0;32m      1\u001B[0m ranges_to_explore \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munits\u001B[39m\u001B[38;5;124m\"\u001B[39m : [\u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m150\u001B[39m],\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124momega\u001B[39m\u001B[38;5;124m\"\u001B[39m : [\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.7\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransient\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;241m100\u001B[39m]\n\u001B[0;32m      8\u001B[0m }\n\u001B[1;32m----> 9\u001B[0m gs \u001B[38;5;241m=\u001B[39m \u001B[43mGridSearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mranges_to_explore\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtr_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdev_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMG17\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m best_config \u001B[38;5;241m=\u001B[39m  gs\u001B[38;5;241m.\u001B[39mbest_config\n",
      "Cell \u001B[1;32mIn[3], line 13\u001B[0m, in \u001B[0;36mGridSearch.__init__\u001B[1;34m(self, parameters_grid, tr, dev, dt_type)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of configurations to try: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(all_configs))\n\u001B[0;32m      8\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03mReturns the performance in each configuration:\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m    rank = a list of results for each configuration\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m    loss = training loss history of the best model\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_configs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# we sort by validation loss\u001B[39;00m\n\u001B[0;32m     16\u001B[0m rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(rank, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m conf: conf[\u001B[38;5;241m2\u001B[39m])\n",
      "Cell \u001B[1;32mIn[3], line 41\u001B[0m, in \u001B[0;36mGridSearch.run\u001B[1;34m(tr, dev, configs, dt_type)\u001B[0m\n\u001B[0;32m     35\u001B[0m dev \u001B[38;5;241m=\u001B[39m make_sequence(dev, \u001B[38;5;241m1\u001B[39m, dt_type)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m tqdm(configs): \u001B[38;5;66;03m# try each configuration\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \n\u001B[0;32m     39\u001B[0m     \u001B[38;5;66;03m# With the same hyperparameter, we perform 3 different \"trainings\" and\u001B[39;00m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;66;03m# we evaluate the configuration as average of this training\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m     losses \u001B[38;5;241m=\u001B[39m \u001B[43mmultiple_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mdev\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     rank\u001B[38;5;241m.\u001B[39mappend((config, \u001B[38;5;28mround\u001B[39m(losses[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem(), \u001B[38;5;241m6\u001B[39m), \u001B[38;5;28mround\u001B[39m(losses[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mitem(), \u001B[38;5;241m6\u001B[39m)))\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;66;03m# we keep the best model\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 68\u001B[0m, in \u001B[0;36mmultiple_train\u001B[1;34m(tr, config, dts, return_model)\u001B[0m\n\u001B[0;32m     60\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     62\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ESN_Seq2Seq(\u001B[38;5;241m1\u001B[39m, hidden_dim\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munits\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     63\u001B[0m                       omega\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124momega\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     64\u001B[0m                       spectral_radius\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspectral\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     65\u001B[0m                       tikhonov\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlambda\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     66\u001B[0m                       leakage_rate\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleakage\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m---> 68\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtransient\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m tr_loss, hs, _ \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mpredict(\u001B[38;5;241m*\u001B[39mtr)\n\u001B[0;32m     72\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(tr_loss)\n",
      "Cell \u001B[1;32mIn[2], line 69\u001B[0m, in \u001B[0;36mESN_Seq2Seq.fit\u001B[1;34m(self, x, y, transient)\u001B[0m\n\u001B[0;32m     66\u001B[0m h_stack, y \u001B[38;5;241m=\u001B[39m h_stack[transient:], y[transient:]\n\u001B[0;32m     68\u001B[0m I \u001B[38;5;241m=\u001B[39m eye(h_stack\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m---> 69\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mWo \u001B[38;5;241m=\u001B[39m \u001B[43mpinv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh_stack\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mh_stack\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtikhonov\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mI\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m@\u001B[39m h_stack\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m y\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_cache \u001B[38;5;241m=\u001B[39m h_stack\n",
      "\u001B[1;31m_LinAlgError\u001B[0m: linalg.svd: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 23)."
     ]
    }
   ],
   "source": [
    "ranges_to_explore = {\n",
    "    \"units\" : [50, 100, 150],\n",
    "    \"omega\" : [0.5, 0.7],\n",
    "    \"spectral\" : [0.8, 0.9],\n",
    "    \"lambda\" : [1e-4, 1e-5],\n",
    "    \"leakage\": [0.3, 0.7],\n",
    "    \"transient\": [100]\n",
    "}\n",
    "gs = GridSearch(ranges_to_explore, tr_dataset, dev_dataset, \"MG17\")\n",
    "best_config =  gs.best_config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T17:36:35.283911600Z",
     "start_time": "2023-06-30T17:36:33.513683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_seq = make_sequence(tr_dataset, 1, \"MG17\")\n",
    "dev_seq = make_sequence(dev_dataset, 1, \"MG17\")\n",
    "ts_seq = make_sequence(ts_dataset, 1, \"MG17\")\n",
    "\n",
    "losses = multiple_train(tr_seq, config=best_config,dts=[dev_seq, ts_seq])\n",
    "\n",
    "print(\"Train Error\", round(losses[0].item(), 6))\n",
    "print(\"Validation Error\", round(losses[1].item(), 6))\n",
    "print(\"Test Error\", round(losses[2].item(), 6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "\n",
    "final_tr = vstack([tr_dataset, dev_dataset])\n",
    "final_tr = make_sequence(final_tr, 1, \"MG17\")\n",
    "\n",
    "losses, model = multiple_train(final_tr, config=best_config, dts=[ts_seq], return_model=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, _, tr_pred = model.predict(*final_tr)\n",
    "_, _, ts_pred = model.predict(*ts_seq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from LAB3_1.utils import show_result\n",
    "\n",
    "show_result(tr_pred, tr_seq[1], ts_pred, ts_seq[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
