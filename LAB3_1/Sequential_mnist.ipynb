{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from LAB3_1.utils import compute_acc, show_loss, Sequential_mnist\n",
    "\n",
    "from typing import Tuple\n",
    "from itertools import product\n",
    "\n",
    "import torch, copy\n",
    "from torch import Tensor,zeros, cuda\n",
    "from torch.nn import Module, RNN, GRU, LSTM, Linear, CrossEntropyLoss\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu = 'cuda' if cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Recurrent_RNN(Module): # Recurrent Neural network\n",
    "    def __init__(self,rnn_type:str, hidden: int, layers:int, bi:bool):\n",
    "        super(Recurrent_RNN, self).__init__()\n",
    "\n",
    "        if rnn_type == \"RNN\":\n",
    "            self.rnn = RNN(1, hidden, num_layers=layers, bidirectional=bi, batch_first=True)\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.rnn = LSTM(1, hidden, num_layers=layers, bidirectional=bi, batch_first=True)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = GRU(1, hidden, num_layers=layers, bidirectional=bi, batch_first=True)\n",
    "\n",
    "        B = 2 if bi else 1\n",
    "        self.readout = Linear(B * hidden, 10)\n",
    "        self.criteria = CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: Tensor, y:Tensor=None) -> Tensor:\n",
    "        self.rnn.flatten_parameters()\n",
    "        out, _ = self.rnn(x)\n",
    "        y_pred =  self.readout(out[:,-1,:]) # take only the last hidden state as (cumulative knowledge)\n",
    "\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = self.criteria(y_pred, y)\n",
    "        return (loss, y_pred) if loss is not None else y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN_trainer:\n",
    "    def __init__(self, rnn_type:str, hidden:int, layers:int, bi:bool):\n",
    "\n",
    "        # model\n",
    "        self.model = Recurrent_RNN(rnn_type, hidden=hidden, layers=layers, bi=bi).to(gpu)\n",
    "\n",
    "    def fit(self, dataset:Dataset, epochs:int=2, lr:float=0.001):\n",
    "\n",
    "        # Build a dataloader with the training dataset\n",
    "        loader = DataLoader(dataset, batch_size=64)\n",
    "        opt = Adam(self.model.parameters(), lr)\n",
    "        history_tr = zeros(epochs) # keep track the loss and accuracy through epochs\n",
    "\n",
    "        loss, y, y_pred =  None, None, None\n",
    "\n",
    "        self.model.train()\n",
    "        for i in range(epochs):\n",
    "\n",
    "            for x, y in loader:\n",
    "                x,y = x.to(gpu), y.to(gpu)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss, y_pred = self.model(x, y)\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(self.model.parameters(), 1)\n",
    "                opt.step()\n",
    "\n",
    "            history_tr[i] = loss.item()\n",
    "            if i % 15 == 0:\n",
    "                print(f'Epoch {i} Loss: {round(loss.item(), 4)} Accuracy {round(compute_acc(y_pred, y), 4)}')\n",
    "\n",
    "        return history_tr\n",
    "\n",
    "    def validate(self, dataset:Dataset) -> Tuple:\n",
    "        # Build a dataloader with the dataset taken (train, validation or test)\n",
    "        loader = DataLoader(dataset, batch_size=64)\n",
    "        cum_loss, cum_acc = 0, 0\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x,y = x.to(gpu), y.to(gpu)\n",
    "                loss, y_pred = self.model(x, y)\n",
    "                cum_loss += loss.item()\n",
    "                cum_acc += compute_acc(y_pred, y)\n",
    "\n",
    "        cum_loss /= len(loader)\n",
    "        cum_acc /= len(loader)\n",
    "\n",
    "        return  cum_loss, cum_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "\n",
    "    def __init__(self, rnn_type:str, parameters_grid:dict, tr:Dataset, dev:Dataset):\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        all_configs = [dict(zip(parameters_grid.keys(), configs)) for configs in product(*parameters_grid.values())]\n",
    "\n",
    "        print(\"Number of configurations to try: \",len(all_configs))\n",
    "        # returns the performance in each configuration, the best model and the history of the loss\n",
    "        rank, best, loss = self.run(tr, dev, all_configs)\n",
    "\n",
    "        # we sort by validation loss\n",
    "        rank = sorted(rank, key=lambda conf: -conf[2])\n",
    "\n",
    "        print(\"\\nThe best solution in \", rank[0])\n",
    "        self.best_config = rank[0][0]\n",
    "        self.best_model = best\n",
    "        self.tr_loss = loss\n",
    "\n",
    "    def run(self, tr:Dataset, dev:Dataset, configs:list):\n",
    "        \"\"\"\n",
    "        In the grid search, we explore all configurations provided and try to find the best\n",
    "        hyperparameter configuration using the training set to train the model and the validation\n",
    "        set to compare the performance among all models instantiated by configurations.\n",
    "        \"\"\"\n",
    "\n",
    "        rank = [] # the keep in track the configuration and the corresponding performance\n",
    "\n",
    "        # we save the best trained model and the training loss during the epochs\n",
    "        best, loss = None, None\n",
    "        best_dev_acc = 0\n",
    "\n",
    "        for idx, config in enumerate(configs):\n",
    "            print(\"Config: \",idx)\n",
    "\n",
    "            trainer = RNN_trainer(rnn_type=self.rnn_type,\n",
    "                                  hidden=config[\"units\"],\n",
    "                                  layers=config[\"layers\"],\n",
    "                                  bi=config[\"bi\"])\n",
    "\n",
    "            history  = trainer.fit(tr, config[\"epochs\"], config[\"lr\"])\n",
    "            _, acc_vl = trainer.validate(dev)\n",
    "\n",
    "            rank.append((config, round(history[-1].item(), 4), round(acc_vl, 4)))\n",
    "\n",
    "            print(f'Results: Acc tr: {round(history[-1].item(), 4)}', f'Acc vl: {round(acc_vl, 4)}')\n",
    "\n",
    "            # we keep the best model\n",
    "            if best_dev_acc < acc_vl:\n",
    "                best_dev_acc = acc_vl\n",
    "                loss = copy.deepcopy(history)\n",
    "                best = copy.deepcopy(trainer)\n",
    "\n",
    "        return rank, best, loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus-Track Assignment 2: Sequential MNIST classification task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve the dataset and Hold out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_dataset = Sequential_mnist(\"train\")\n",
    "dev_dataset = Sequential_mnist(\"dev\")\n",
    "ts_dataset = Sequential_mnist(\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search Vanilla RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranges_to_explore = {\n",
    "    \"units\" : [10, 20],\n",
    "    \"epochs\" : [100],\n",
    "    \"lr\" : [0.001, 0.004],\n",
    "    \"layers\": [1, 2],\n",
    "    \"bi\" : [True]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs = GridSearch(\"RNN\",ranges_to_explore, tr_dataset, dev_dataset)\n",
    "best_config =  gs.best_config\n",
    "best_model = gs.best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(gs.tr_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_model.validate(tr_dataset)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "dev_loss, dev_acc = best_model.validate(dev_dataset)\n",
    "print(f'Train loss: {round(dev_loss, 6)}', f'Accuracy: {round(dev_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc = best_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Final retrain with Training and Validation set (with the best configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_rrn_model = RNN_trainer(rnn_type=\"RNN\",\n",
    "                            hidden=best_config[\"units\"],\n",
    "                            layers=best_config[\"layers\"],\n",
    "                            bi=best_config[\"bi\"])\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "final_tr = Sequential_mnist(\"train-dev\")\n",
    "tr_history, tr_hist_acc = best_rrn_model.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])\n",
    "\n",
    "tr_loss, tr_acc = best_rrn_model.validate(final_tr)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc  = best_rrn_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(tr_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus Track Assignment 4: benchmarking RNN models on the sequential MNIST task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs = GridSearch(\"LSTM\",ranges_to_explore, tr_dataset, dev_dataset)\n",
    "best_config =  gs.best_config\n",
    "best_model = gs.best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_model.validate(tr_dataset)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "dev_loss, dev_acc = best_model.validate(dev_dataset)\n",
    "print(f'Train loss: {round(dev_loss, 6)}', f'Accuracy: {round(dev_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc = best_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_lstm_model = RNN_trainer(rnn_type=\"LSTM\",\n",
    "                             hidden=best_config[\"units\"],\n",
    "                             layers=best_config[\"layers\"],\n",
    "                             bi=best_config[\"bi\"])\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "final_tr = Sequential_mnist(\"train-dev\")\n",
    "best_lstm_model.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])\n",
    "\n",
    "tr_loss, tr_acc = best_lstm_model.validate(final_tr)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc  = best_lstm_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search GRU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs = GridSearch(\"GRU\",ranges_to_explore, tr_dataset, dev_dataset)\n",
    "best_config =  gs.best_config\n",
    "best_model = gs.best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_model.validate(tr_dataset)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "dev_loss, dev_acc = best_model.validate(dev_dataset)\n",
    "print(f'Train loss: {round(dev_loss, 6)}', f'Accuracy: {round(dev_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc = best_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_gru_model = RNN_trainer(rnn_type=\"GRU\",\n",
    "                             hidden=best_config[\"units\"],\n",
    "                             layers=best_config[\"layers\"],\n",
    "                             bi=best_config[\"bi\"])\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "final_tr = Sequential_mnist(\"train-dev\")\n",
    "best_gru_model.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])\n",
    "\n",
    "tr_loss, tr_acc = best_gru_model.validate(final_tr)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc  = best_gru_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
