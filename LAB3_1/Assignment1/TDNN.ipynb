{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1.1: NARMA10 task with TDNN\n",
    "This task consists of predicting the output of a 10th order non-linear autoregressive moving average (NARMA) system.\n",
    "Given the input value x(t), the task is to predict the corresponding value of y(t).\n",
    "* Import the dataset from the .csv file “NARMA10.csv” (available on the Moodle platform), where the first row represents the input, and the second row represents the target output. Different columns represent different time-steps.\n",
    "* Split the data into training (the first 5000 time steps), and test set (remaining time steps). Note that for model selection you will use the data in the training set, with a further split in training (first 4000 samples) and validation (last 1000 samples).\n",
    "* For the sake of problem understanding, you can try to first visualize the time-series data (using the matplotlib library in Python or the plot command in Matlab)\n",
    "\n",
    "Remember: when training an RNN, you want to make sure to keep the last hidden state of your RNN after the training session, and use it as initial hidden state of the validation session. The same applies when transitioning to the test session.\n",
    "\n",
    "1) Create and train a Time Delay Neural Network (TDNN) to solve the task, considering the fundamental hyperparameters (e.g., length of the input delay, training function/optimizer, learning rate, number of training epochs, etc.).\n",
    "\n",
    "2) Perform a model selection (e.g., by grid search or random search) on the values of the hyperparameters identified in the previous point. Select on the validation set the best hyper-parametrization, as the one with the smallest Mean Squared Error (MSE).\n",
    "\n",
    "3) Train the TDNN model with the selected hyper-parametrization on the whole training set, and evaluate its MSE on the training set and on the test set.\n",
    "\n",
    "4) Repeat steps (1)-(3) also for Recurrent Neural Network (RNN), considering in this case the appropriate hyperparameters (e.g., in this case you don’t have an input delay line)\n",
    "\n",
    "# Bonus-track Assignment 1.2: Mackey-Glass 17 task with TDNN\n",
    "\n",
    "The data for this task is available in the file “MG17.csv” (available on the Moodle platform), and it consists of one single row containing the values of the Q(t) time-series for all the time-steps in the different columns. The task consists in predicting 1 step in the future the Mackey-Glass dynamical system, i.e. predicting the value Q(t+1), just looking at Q(t) (or at a tapped delay line of order s, i.e. Q(t),...,Q(t-s), for a TDNN). Before splitting the data into training, validation and test splits, you need to organize it into input and target information. Specifically, the input should consist of the time-series values from the first time-step to the second-to-last, and the target in the time-series value from the second time-step to the last."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys, copy, torch, json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy import ndarray, vstack\n",
    "from typing import Tuple\n",
    "from itertools import product\n",
    "\n",
    "from torch import Tensor, no_grad, zeros\n",
    "from torch.optim import Adam\n",
    "\n",
    "# To work with Google colab\n",
    "!wget https://raw.githubusercontent.com/jacons/Computational-NeuroScience-Lab/master/Utils/utils.py\n",
    "!wget https://raw.githubusercontent.com/jacons/Computational-NeuroScience-Lab/master/LAB3_1/Assignment1/models.py\n",
    "\n",
    "from Utils.utils import show_split, show_result, show_loss, make_sequence\n",
    "from LAB3_1.Assignment1.models import TimeDelayNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To work with Google colab\n",
    "#!wget https://raw.githubusercontent.com/jacons/Computational-NeuroScience-Lab/master/Sources/NARMA10.csv\n",
    "#!wget https://raw.githubusercontent.com/jacons/Computational-NeuroScience-Lab/master/Sources/MG17.csv\n",
    "source1 = pd.read_csv(\"./../../Sources/NARMA10.csv\", header=None).T.to_numpy()\n",
    "source2 = pd.read_csv(\"./../../Sources/MG17.csv\", header=None).T.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve the datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer and Grid-search function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TDNN_trainer:\n",
    "    def __init__(self,units:int, back_steps:int, dt_type:str):\n",
    "\n",
    "        self.model = TimeDelayNN(units, back_steps) # model\n",
    "        self.back_steps = back_steps\n",
    "        self.dt_type = dt_type\n",
    "\n",
    "    def fit(self, df:ndarray, epochs:int=2, lr:float=0.001)->Tensor:\n",
    "        \"\"\"\n",
    "        Give the raw dataset dt, the number of epochs and the learning rate.\n",
    "        It fits the model and returns the training loss history.\n",
    "        \"\"\"\n",
    "\n",
    "        # Build sequences like [x1,x2,x3] -> [x4] , [x2,x3,x4] -> [x5] and so on\n",
    "        x, y = make_sequence(df, self.back_steps + 1, self.dt_type)\n",
    "\n",
    "        opt = Adam(self.model.parameters(), lr)\n",
    "        history_tr = zeros(epochs) # Keep track the behavior of loss\n",
    "\n",
    "        self.model.train()\n",
    "        for i in range(epochs):\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss, _ = self.model(x, y) # perform the output\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # save the current loss\n",
    "            history_tr[i] = loss.item()\n",
    "\n",
    "        return history_tr # return the history of loss\n",
    "\n",
    "    def validate(self, df:ndarray) -> Tuple:\n",
    "        \"\"\"\n",
    "        Given a validation dataset, it performs the loss\n",
    "        \"\"\"\n",
    "        x, y = make_sequence(df, self.back_steps + 1, self.dt_type)\n",
    "        return self.predict(x, y) + (y,)\n",
    "\n",
    "    def predict(self, x:Tensor, y:Tensor=None):\n",
    "        \"\"\"\n",
    "        Predict the output of a certain input, if the target it is provided, the method performs also the loss,\n",
    "        otherwise return only the output of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        with no_grad():\n",
    "            return self.model(x, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "\n",
    "    def __init__(self, parameters_grid:dict, tr:ndarray, dev:ndarray, dt_type:str):\n",
    "\n",
    "        all_configs = [dict(zip(parameters_grid.keys(), configs)) for configs in product(*parameters_grid.values())]\n",
    "\n",
    "        print(\"Number of configurations to try: \",len(all_configs))\n",
    "\n",
    "        \"\"\"\n",
    "        Returns the performance in each configuration:\n",
    "\n",
    "            rank = a list of results for each configuration\n",
    "            best = best model used to final retrain\n",
    "            loss = training loss history of the best model\n",
    "        \"\"\"\n",
    "        rank, best, loss = self.run(tr, dev, all_configs, dt_type)\n",
    "\n",
    "        # we sort by validation loss\n",
    "        rank = sorted(rank, key=lambda conf: conf[2])\n",
    "\n",
    "        print(\"\\nThe best solution in \", rank[0])\n",
    "        self.best_config = rank[0][0]\n",
    "        self.best_model = best # the best model discovered\n",
    "        self.tr_loss = loss # loss history\n",
    "\n",
    "    @staticmethod\n",
    "    def run(tr:ndarray, dev:ndarray, configs:list, dt_type:str):\n",
    "        \"\"\"\n",
    "        In the grid search, we explore all configurations provided and try to find the best\n",
    "        hyperparameter configuration using the training set to train the model and the validation\n",
    "        set to compare the performance among all models instantiated by configurations.\n",
    "        \"\"\"\n",
    "\n",
    "        rank = [] # keep in track the configuration and the corresponding performance\n",
    "\n",
    "        # we save the best trained model and the training loss history during the epochs\n",
    "        best, loss = None, None\n",
    "        best_dev_loss = sys.maxsize\n",
    "\n",
    "        for config in tqdm(configs): # try each configuration\n",
    "\n",
    "            trainer = TDNN_trainer(units=config[\"units\"],\n",
    "                                   back_steps=config[\"b_step\"],\n",
    "                                   dt_type=dt_type)\n",
    "\n",
    "            history = trainer.fit(tr, config[\"epochs\"], config[\"lr\"])\n",
    "            vl_loss = trainer.validate(dev)[0].item()\n",
    "\n",
    "            rank.append((config, round(history[-1].item(), 6), round(vl_loss, 6)))\n",
    "\n",
    "            # we keep the best model\n",
    "            if best_dev_loss > vl_loss:\n",
    "                best_dev_loss = vl_loss\n",
    "                loss = copy.deepcopy(history)\n",
    "                best = copy.deepcopy(trainer)\n",
    "\n",
    "        return rank, best, loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NARMA10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hold-out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_dataset, dev_dataset, ts_dataset = source1[:4000], source1[4000:5000], source1[5000:]\n",
    "# Although is not much representative, we plot the time target\n",
    "show_split(tr_dataset, dev_dataset, ts_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_dictionary = {\"NARMA\": {}, \"MG17\": {}}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranges_to_explore = {\n",
    "    \"units\" : [5, 10, 20, 50, 100, 500],\n",
    "    \"epochs\" : [10, 20, 50, 100],\n",
    "    \"lr\" : [0.0005, 0.001, 0.003, 0.005],\n",
    "    \"b_step\" : [5, 8, 10, 12, 15, 20]\n",
    "}\n",
    "\n",
    "gs = GridSearch(ranges_to_explore, tr_dataset, dev_dataset, \"NARMA10\")\n",
    "best_config = gs.best_config\n",
    "best_model = gs.best_model\n",
    "\n",
    "json_dictionary[\"NARMA\"][\"best_config\"] = best_config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(gs.tr_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train, Validation and Test errors in the best configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss = best_model.validate(tr_dataset)[0]\n",
    "print(\"Train Error\", round(tr_loss.item(), 6))\n",
    "\n",
    "dev_loss = best_model.validate(dev_dataset)[0]\n",
    "print(\"Validation Error\", round(dev_loss.item(), 6))\n",
    "\n",
    "ts_loss = best_model.validate(ts_dataset)[0]\n",
    "print(\"Test Error\", round(ts_loss.item(), 6))\n",
    "\n",
    "json_dictionary[\"NARMA\"][\"best_config\"] = best_config\n",
    "json_dictionary[\"NARMA\"][\"Model_evaluation\"] = (\n",
    "    round(tr_loss.item(),6),\n",
    "    round(dev_loss.item(),6),\n",
    "    round(ts_loss.item(),6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final retrain with Training and Validation set (with the best configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_trainer = TDNN_trainer(units=best_config[\"units\"],\n",
    "                             back_steps=best_config[\"b_step\"],\n",
    "                             dt_type=\"NARMA10\")\n",
    "\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "final_tr = vstack([tr_dataset, dev_dataset])\n",
    "tr_history = final_trainer.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])\n",
    "\n",
    "tr_loss, tr_pred, tr_y = final_trainer.validate(final_tr)\n",
    "print(\"Training Error\", round(tr_loss.item(), 6))\n",
    "\n",
    "ts_loss, ts_pred, test_y = final_trainer.validate(ts_dataset)\n",
    "print(\"Test Error\", round(ts_loss.item(), 6))\n",
    "\n",
    "json_dictionary[\"NARMA\"][\"Final_retrain\"] = (\n",
    "    round(tr_loss.item(),6),\n",
    "    round(ts_loss.item(),6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(tr_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_result(tr_pred, tr_y, ts_pred, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save([best_model.model, gs.tr_loss, tr_pred, tr_y, ts_pred, test_y],\"caches/narma_tdnn.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mackey-Glass 17"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hold out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_dataset, dev_dataset, ts_dataset = source2[:4000], source2[4000:5000], source2[5000:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranges_to_explore = {\n",
    "    \"units\" : [5, 10, 20, 50, 100, 500],\n",
    "    \"epochs\" : [10, 20, 50, 100],\n",
    "    \"lr\" : [0.0005, 0.001, 0.003, 0.005],\n",
    "    \"b_step\" : [5, 8, 10, 12, 15, 20]\n",
    "}\n",
    "\n",
    "gs = GridSearch(ranges_to_explore, tr_dataset, dev_dataset, \"MG17\")\n",
    "best_config = gs.best_config\n",
    "best_model = gs.best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(gs.tr_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train, Validation and Test errors in the best configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss = best_model.validate(tr_dataset)[0]\n",
    "print(\"Train Error\", round(tr_loss.item(), 6))\n",
    "\n",
    "dev_loss = best_model.validate(dev_dataset)[0]\n",
    "print(\"Validation Error\", round(dev_loss.item(), 6))\n",
    "\n",
    "ts_loss = best_model.validate(ts_dataset)[0]\n",
    "print(\"Test Error\", round(ts_loss.item(), 6))\n",
    "\n",
    "json_dictionary[\"MG17\"][\"best_config\"] = best_config\n",
    "json_dictionary[\"MG17\"][\"Model_evaluation\"] = (\n",
    "    round(tr_loss.item(),6),\n",
    "    round(dev_loss.item(),6),\n",
    "    round(ts_loss.item(),6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final retrain with Training and Validation set (with the best configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_trainer = TDNN_trainer(units=best_config[\"units\"],\n",
    "                             back_steps=best_config[\"b_step\"],\n",
    "                             dt_type=\"MG17\")\n",
    "\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "final_tr = vstack([tr_dataset, dev_dataset])\n",
    "tr_history = final_trainer.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])\n",
    "\n",
    "tr_loss, tr_pred, tr_y = final_trainer.validate(final_tr)\n",
    "print(\"Training Error\", round(tr_loss.item(), 6))\n",
    "\n",
    "ts_loss, ts_pred, test_y = final_trainer.validate(ts_dataset)\n",
    "print(\"Test Error\", round(ts_loss.item(), 6))\n",
    "\n",
    "json_dictionary[\"MG17\"][\"Final_retrain\"] = (\n",
    "    round(tr_loss.item(),6),\n",
    "    round(ts_loss.item(),6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(tr_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_result(tr_pred, tr_y, ts_pred, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save([best_model.model,gs.tr_loss, tr_pred, tr_y, ts_pred, test_y],\"caches/mg17_tdnn.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"caches/metric_tdnn.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(json_dictionary))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
