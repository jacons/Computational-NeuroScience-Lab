{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus-Track Assignment 2: Sequential MNIST classification task\n",
    "\n",
    "The MNIST dataset is a common dataset used in computer vision. It consists of 60000 images of 28x28 pixels of handwritten digits from 0 to 9. A strategy is to flatten the 28x28 pixels into 784 dimensional vectors and feed them as input to a multilayer perceptron. The Sequential MNIST task (sMNIST) is a challenging task devised to test the ability of an RNN model to learn long-term dependencies. The sMNIST consists of considering the flattened MNIST vectors as a time series of 784 time steps, i.e. we feed an RNN with one pixel per time step starting from the top left of the image ending to the bottom right. After 784 time steps the RNN provides a 10-dimensional output containing the softmax probabilities of the input being one of the 10 digits. NB: you don’t have to produce an output for each time step, but rather you just need to use the last hidden state to perform the classification. The rationale is that the RNN, after “listening” to the whole time series, has encoded in its hidden state the information necessary to classify the time series.\n",
    "\n",
    "# Bonus Track Assignment 4: benchmarking RNN models on the sequential MNIST task\n",
    "\n",
    "The sequential MNIST can be downloaded as described in the Lab 3—bonus track assignment #2. The goal of this assignment is to compare the performance-achievable by various RNN models on this task. As such:\n",
    "\n",
    "1) Write your custom code that solves the sequential MNIST time-series classification task with several recurrent neural networks. You must consider at least: a vanilla RNN model, a gated recurrent neural network (e.g., GRU and/or LSTM). Optionally, you can implement / use a bidirectional RNN, a deep RNN, and an Antisymmetric RNN (as described in lecture slides “Part3_Lecture2.pdf”).\n",
    "2) For every model, identify a suitable set of hyper-parameters and their possible values to explore by model selection\n",
    "3) Perform model selection, choosing, for each model individually, the values of the hyper-parameters on the validation set.\n",
    "4) For each model individually, train the network with the chosen configuration on the whole training set, and assess it on the test set. For each model, run a number of Ng identical experiments (same hyperparameters’ values, different random seeds), where Ng >= 5, and record the training and test accuracies achieved. Compute averages and std over the Ng repetitions for each model individually.\n",
    "5) Provide a Table in which you report the training and test statistics of the achieved results (computed as indicated in the previous point)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import copy, os, json\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import cuda, zeros\n",
    "from torch.nn import RNN, LSTM, GRU\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# To work with Google colab\n",
    "#!wget https://raw.githubusercontent.com/jacons/Computational-NeuroScience-Lab/master/LAB3_1/utils.py\n",
    "#!wget https://raw.githubusercontent.com/jacons/Computational-NeuroScience-Lab/master/LAB3_1/Assignment2_4/model.py\n",
    "\n",
    "from LAB3_1.utils import compute_acc, Sequential_mnist , show_loss\n",
    "from LAB3_1.Assignment2_4.model import sMNIST_model"
   ],
   "metadata": {
    "id": "auoVhkKl_FmG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu = 'cuda' if cuda.is_available() else 'cpu'\n",
    "if not os.path.exists('caches'):\n",
    "    os.makedirs('caches')"
   ],
   "metadata": {
    "id": "PmJtmlF7Nct5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer and Grid-search function"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Mube3U96Nct9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN_trainer:\n",
    "    def __init__(self, rnn_type:str, hidden:int, layers:int, bi:bool):\n",
    "\n",
    "        if rnn_type == \"RNN\":\n",
    "            rnn = RNN(1, hidden, num_layers=layers, bidirectional=bi, batch_first=True)\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            rnn = LSTM(1, hidden, num_layers=layers, bidirectional=bi, batch_first=True)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            rnn = GRU(1, hidden, num_layers=layers, bidirectional=bi, batch_first=True)\n",
    "        else:\n",
    "            rnn = None\n",
    "\n",
    "        # model\n",
    "        self.model = sMNIST_model(rnn, hidden=hidden, bi=bi).to(gpu)\n",
    "\n",
    "    def fit(self, ds:Dataset, epochs:int=2, lr:float=0.001):\n",
    "        \"\"\"\n",
    "        Give the raw dataset ds, the number of epochs and the learning rate.\n",
    "        It fits the model and returns the training loss history.\n",
    "        \"\"\"\n",
    "\n",
    "        # Build a dataloader with the training dataset\n",
    "        loader = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "        opt = Adam(self.model.parameters(), lr)\n",
    "        history_tr = [] # Keep track the behavior of loss\n",
    "\n",
    "        loss, y, y_pred =  None, None, None\n",
    "\n",
    "        self.model.train()\n",
    "        for i in range(epochs):\n",
    "\n",
    "            for x, y in loader:\n",
    "                x,y = x.to(gpu), y.to(gpu) # perform the output\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss, y_pred = self.model(x, y)\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(self.model.parameters(), 0.8)\n",
    "                opt.step()\n",
    "\n",
    "            if compute_acc(y_pred, y) >= 0.98:\n",
    "                break\n",
    "            history_tr.append(loss.item())\n",
    "\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\".\")\n",
    "            else:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        print(\"\")\n",
    "        return torch.tensor(history_tr)\n",
    "\n",
    "    def validate(self, ds:Dataset) -> Tuple:\n",
    "        \"\"\"\n",
    "        Given a validation dataset, it performs the loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Build a dataloader with the dataset taken\n",
    "        loader = DataLoader(ds, batch_size=512)\n",
    "        cum_loss, cum_acc = 0, 0\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x,y = x.to(gpu), y.to(gpu)\n",
    "                loss, y_pred = self.model(x, y)\n",
    "                cum_loss += loss.item()\n",
    "                cum_acc += compute_acc(y_pred, y)\n",
    "\n",
    "        cum_loss /= len(loader)\n",
    "        cum_acc /= len(loader)\n",
    "\n",
    "        return  cum_loss, cum_acc"
   ],
   "metadata": {
    "id": "xLZ-dyw_Nct-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "\n",
    "    def __init__(self, rnn_type:str, parameters_grid:dict, tr:Dataset, dev:Dataset):\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        all_configs = [dict(zip(parameters_grid.keys(), configs)) for configs in product(*parameters_grid.values())]\n",
    "\n",
    "        print(\"Number of configurations to try: \",len(all_configs))\n",
    "\n",
    "        \"\"\"\n",
    "        Returns the performance in each configuration:\n",
    "\n",
    "            rank = a list of results for each configuration\n",
    "            best = best model used to final retrain\n",
    "            loss = training loss history of the best model\n",
    "        \"\"\"\n",
    "        rank, best, loss = self.run(tr, dev, all_configs)\n",
    "\n",
    "        # we sort by validation loss\n",
    "        rank = sorted(rank, key=lambda conf: -conf[2])\n",
    "\n",
    "        print(\"\\nThe best solution in \", rank[0])\n",
    "        self.best_config = rank[0][0]\n",
    "        self.best_model = best\n",
    "        self.tr_loss = loss\n",
    "\n",
    "    def run(self, tr:Dataset, dev:Dataset, configs:list):\n",
    "        \"\"\"\n",
    "        In the grid search, we explore all configurations provided and try to find the best\n",
    "        hyperparameter configuration using the training set to train the model and the validation\n",
    "        set to compare the performance among all models instantiated by configurations.\n",
    "        \"\"\"\n",
    "\n",
    "        rank = [] # the keep in track the configuration and the corresponding performance\n",
    "\n",
    "        # we save the best trained model and the training loss during the epochs\n",
    "        best, loss = None, None\n",
    "        best_dev_acc = 0\n",
    "\n",
    "        for idx, config in enumerate(configs):\n",
    "            print(\"Config: \",idx)\n",
    "\n",
    "            trainer = RNN_trainer(self.rnn_type, hidden=config[\"units\"],\n",
    "                                  layers=config[\"layers\"], bi=config[\"bi\"])\n",
    "\n",
    "            history  = trainer.fit(tr, config[\"epochs\"], config[\"lr\"])\n",
    "            _, acc_vl = trainer.validate(dev)\n",
    "\n",
    "            rank.append((config, round(history[-1].item(), 4), round(acc_vl, 4)))\n",
    "\n",
    "            print(f'Results: Acc tr: {round(history[-1].item(), 4)}', f'Acc vl: {round(acc_vl, 4)}')\n",
    "\n",
    "            # we keep the best model\n",
    "            if best_dev_acc < acc_vl:\n",
    "                best_dev_acc = acc_vl\n",
    "                loss = copy.deepcopy(history)\n",
    "                best = copy.deepcopy(trainer)\n",
    "\n",
    "        return rank, best, loss"
   ],
   "metadata": {
    "id": "dxyPA69FNcuA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve the dataset and Hold out"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dxVPQ8MvNcuD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_dataset = Sequential_mnist(\"train\", root=\"..\\sources\\MNIST\")\n",
    "dev_dataset = Sequential_mnist(\"dev\", root=\"..\\sources\\MNIST\")\n",
    "ts_dataset = Sequential_mnist(\"test\", root=\"..\\sources\\MNIST\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTb2HiboNcuE",
    "outputId": "1c47da1e-2927-4607-b92e-4952d9570973"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_dictionary = {}\n",
    "save_best_models = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search Vanilla RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranges_to_explore = {\n",
    "    \"units\" : [10, 20],\n",
    "    \"epochs\" : [200],\n",
    "    \"lr\" : [0.001, 0.004],\n",
    "    \"layers\": [1, 2],\n",
    "    \"bi\" : [True]\n",
    "}"
   ],
   "metadata": {
    "id": "9Hbxerb4NcuG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs = GridSearch(\"RNN\", ranges_to_explore, tr_dataset, dev_dataset)\n",
    "best_config =  gs.best_config\n",
    "best_model = gs.best_model\n",
    "\n",
    "json_dictionary[\"RNN\"] = best_config"
   ],
   "metadata": {
    "id": "cacsk8M6b2Kv",
    "outputId": "b31eadb6-35ab-4eb5-8f8e-04d03e81c6f1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(gs.tr_loss)"
   ],
   "metadata": {
    "id": "F6LAF_DEb2Kv",
    "outputId": "19aa897d-2095-40fe-fb42-3a6787b856c8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_model.validate(tr_dataset)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "dev_loss, dev_acc = best_model.validate(dev_dataset)\n",
    "print(f'Train loss: {round(dev_loss, 6)}', f'Accuracy: {round(dev_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc = best_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZId4iAnjb2Kw",
    "outputId": "bac7b7c3-6d8f-4d8b-eaf7-c87a3701057f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Final retrain with Training and Validation set (with the best configuration)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "piUWiq7ANcuJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_rrn_model = RNN_trainer(\"RNN\", hidden=best_config[\"units\"],\n",
    "                            layers=best_config[\"layers\"], bi=best_config[\"bi\"])\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "final_tr = Sequential_mnist(\"train-dev\", root=\"..\\sources\\MNIST\")\n",
    "tr_history = best_rrn_model.fit(final_tr, best_config[\"epochs\"] * 2, lr=best_config[\"lr\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_rrn_model.validate(final_tr)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc  = best_rrn_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(tr_history)"
   ],
   "metadata": {
    "id": "wBupMz_ob2Kx",
    "outputId": "9cb8d4e9-53b2-498c-e7a4-2a68869a7547",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_best_models.append(best_rrn_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search LSTM"
   ],
   "metadata": {
    "collapsed": false,
    "id": "BySrE_FNNcuL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs = GridSearch(\"LSTM\",ranges_to_explore, tr_dataset, dev_dataset)\n",
    "best_config =  gs.best_config\n",
    "best_model = gs.best_model\n",
    "\n",
    "json_dictionary[\"LSTM\"] = best_config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qez_302MNcuL",
    "outputId": "69cb7039-c7c1-4e78-b873-d3dc47b1bc6f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_model.validate(tr_dataset)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "dev_loss, dev_acc = best_model.validate(dev_dataset)\n",
    "print(f'Train loss: {round(dev_loss, 6)}', f'Accuracy: {round(dev_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc = best_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "943adUieNcuL",
    "outputId": "2d4ce7b6-a5fb-4a7b-ce81-f193b5b5fe3d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Final retrain with Training and Validation set (with the best configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_lstm_model = RNN_trainer(\"LSTM\", hidden=best_config[\"units\"],\n",
    "                             layers=best_config[\"layers\"], bi=best_config[\"bi\"])\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "tr_history = best_lstm_model.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9jfXH75NcuL",
    "outputId": "565e0d0b-0bb7-465a-e5bc-b7a74b49e255"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tr_loss, tr_acc = best_lstm_model.validate(final_tr)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc  = best_lstm_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "id": "KtUZ1YNikUFc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(tr_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_best_models.append(best_lstm_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search GRU"
   ],
   "metadata": {
    "collapsed": false,
    "id": "kL_tZzSmNcuM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs = GridSearch(\"GRU\",ranges_to_explore, tr_dataset, dev_dataset)\n",
    "best_config =  gs.best_config\n",
    "best_model = gs.best_model\n",
    "\n",
    "json_dictionary[\"GRU\"] = best_config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kB6YGhrgNcuM",
    "outputId": "db9194ae-7914-4ab9-c423-e962b052413d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_model.validate(tr_dataset)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "dev_loss, dev_acc = best_model.validate(dev_dataset)\n",
    "print(f'Train loss: {round(dev_loss, 6)}', f'Accuracy: {round(dev_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc = best_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Blwv0MRYNcuN",
    "outputId": "f9257228-8161-4279-bece-4969adc3e436"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Final retrain with Training and Validation set (with the best configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_gru_model = RNN_trainer(\"GRU\",hidden=best_config[\"units\"],\n",
    "                             layers=best_config[\"layers\"], bi=best_config[\"bi\"])\n",
    "# we use both training and validation as a training set, using the best parameters\n",
    "# found in the previous model selection\n",
    "tr_history = best_gru_model.fit(final_tr, best_config[\"epochs\"], lr=best_config[\"lr\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFK6uUCxNcuN",
    "outputId": "20e6611c-4985-486e-d76a-335512764ada"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_loss, tr_acc = best_gru_model.validate(final_tr)\n",
    "print(f'Train loss: {round(tr_loss, 6)}', f'Accuracy: {round(tr_acc, 3)}')\n",
    "\n",
    "test_loss, test_acc  = best_gru_model.validate(ts_dataset)\n",
    "print(f'Train loss: {round(test_loss, 6)}', f'Accuracy: {round(test_acc, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_loss(tr_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_best_models.append(best_gru_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(save_best_models,\"caches/best_models.pt\")\n",
    "with open(\"caches/metrics_rnn_lstm_gru.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(json_dictionary))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the statistical table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def experiments(name:str, n:int):\n",
    "\n",
    "    losses_tr, accuracies_tr = torch.zeros(n), torch.zeros(n)\n",
    "    losses_ts, accuracies_ts = torch.zeros(n), torch.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        print(\"Experiment \", i)\n",
    "        torch.random.manual_seed(torch.randint(1,10000,(1,)).item())\n",
    "        bc = json_dictionary[name]\n",
    "\n",
    "        trainer = RNN_trainer(name, hidden=bc[\"units\"], layers=bc[\"layers\"], bi=bc[\"bi\"])\n",
    "        trainer.fit(tr_dataset, bc[\"epochs\"], bc[\"lr\"])\n",
    "\n",
    "        loss_tr, acc_tr = trainer.validate(tr_dataset)\n",
    "        loss_ts, acc_ts = trainer.validate(ts_dataset)\n",
    "\n",
    "        losses_tr[i], losses_ts[i] = loss_tr, loss_ts\n",
    "        accuracies_tr[i], accuracies_ts[i] = acc_tr, acc_ts\n",
    "\n",
    "    results = {\"training_loss\": (losses_tr.mean(), losses_tr.std()),\n",
    "               \"test_loss\": (losses_ts.mean(), losses_ts.std()),\n",
    "               \"training_acc\": (accuracies_tr.mean(), accuracies_tr.std()),\n",
    "               \"test_acc\": (accuracies_ts.mean(), accuracies_ts.std())}\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "table_result = {\n",
    "    \"results_rnn\": experiments(\"RNN\", 5),\n",
    "    \"results_lstm\": experiments(\"LSTM\", 5),\n",
    "    \"results_gru\":  experiments(\"GRU\", 5)\n",
    "}\n",
    "with open(\"caches/table_result.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(json_dictionary))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
